{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(a * b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some operations\n",
    "add = tf.add(a, b)\n",
    "mul = tf.multiply(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(mul, feed_dict={a: 40, b: 50}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = tf.constant([[2, 5], [5, 1]])\n",
    "matrix2 = tf.constant([[1], [-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = tf.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8]\n",
      " [ 3]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    result = sess.run(prod)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 1000\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = '/tmp/tensorflow_logs/example/'\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Input\"):\n",
    "    X = tf.placeholder('float')\n",
    "    Y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Parameters\"):\n",
    "    W = tf.Variable(rng.randn(), name=\"weight\")\n",
    "    b = tf.Variable(rng.randn(), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"LinearRegression\"):\n",
    "    pred = tf.add(tf.multiply(X,W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Loss\"):\n",
    "    loss = tf.reduce_sum(tf.pow(pred - Y, 2)/(2*n_samples))\n",
    "with tf.name_scope(\"SGD\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "logs_path = '/tmp/tensorflow_logs/example/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 0.234031364 W= 0.09558651 b= 1.3457171\n",
      "Epoch: 0100 cost= 0.097468473 W= 0.16532864 b= 1.3521179\n",
      "Epoch: 0150 cost= 0.095746644 W= 0.17301163 b= 1.3497789\n",
      "Epoch: 0200 cost= 0.095501840 W= 0.17423233 b= 1.3465468\n",
      "Epoch: 0250 cost= 0.095277146 W= 0.17477788 b= 1.3432393\n",
      "Epoch: 0300 cost= 0.095055573 W= 0.17525016 b= 1.3399432\n",
      "Epoch: 0350 cost= 0.094836354 W= 0.17571323 b= 1.3366609\n",
      "Epoch: 0400 cost= 0.094620146 W= 0.17617233 b= 1.3334041\n",
      "Epoch: 0450 cost= 0.094406769 W= 0.17662814 b= 1.33017\n",
      "Epoch: 0500 cost= 0.094195627 W= 0.17708108 b= 1.3269504\n",
      "Epoch: 0550 cost= 0.093986921 W= 0.17753185 b= 1.3237488\n",
      "Epoch: 0600 cost= 0.093780823 W= 0.17797971 b= 1.3205675\n",
      "Epoch: 0650 cost= 0.093577698 W= 0.17842472 b= 1.3174136\n",
      "Epoch: 0700 cost= 0.093377441 W= 0.1788656 b= 1.314285\n",
      "Epoch: 0750 cost= 0.093178883 W= 0.17930487 b= 1.3111639\n",
      "Epoch: 0800 cost= 0.092982799 W= 0.17974138 b= 1.308063\n",
      "Epoch: 0850 cost= 0.092788614 W= 0.18017608 b= 1.304974\n",
      "Epoch: 0900 cost= 0.092596881 W= 0.18060842 b= 1.3019046\n",
      "Epoch: 0950 cost= 0.092408195 W= 0.18103713 b= 1.2988656\n",
      "Epoch: 1000 cost= 0.092221543 W= 0.18146278 b= 1.2958416\n",
      "Training cost= 0.09222154 W= 0.18146278 b= 1.2958416 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "     # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(loss, feed_dict={X: train_X, Y:train_Y})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
    "                \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "    training_cost = sess.run(loss, feed_dict={X: train_X, Y: train_Y})\n",
    "    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
    "\n",
    "    #Graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"int32\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(X):\n",
    "    hidden_layer_1 = tf.add(tf.matmul(X, weights['h1']), biases['b1'])\n",
    "    hidden_layer_2 = tf.add(tf.matmul(hidden_layer_1, weights['h2']), biases['b2'])\n",
    "    output = tf.add(tf.matmul(hidden_layer_2, weights['out']), biases['out'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_net = neural_net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits_net, labels=Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True,\n",
    "                source_url='http://yann.lecun.com/exdb/fashion_mnist/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits_net, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 8948.6719, Training Accuracy= 0.461\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument 8948.672 has invalid type <class 'numpy.float32'>, must be a string or Tensor. (Can not convert a float32 into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/graphvite/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    304\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 305\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    306\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graphvite/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3606\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3607\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graphvite/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3695\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" %\n\u001b[0;32m-> 3696\u001b[0;31m                       (type(obj).__name__, types_str))\n\u001b[0m\u001b[1;32m   3697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a float32 into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-42598afb93fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m# Calculate batch loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             loss, acc = sess.run([loss, accuracy], feed_dict={X: batch_x,\n\u001b[0;32m---> 11\u001b[0;31m                                                                  Y: batch_y})\n\u001b[0m\u001b[1;32m     12\u001b[0m             print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n\u001b[1;32m     13\u001b[0m                   \u001b[0;34m\"{:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", Training Accuracy= \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graphvite/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graphvite/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1165\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graphvite/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \"\"\"\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graphvite/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections_abc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graphvite/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \"\"\"\n\u001b[1;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graphvite/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \"\"\"\n\u001b[1;32m    374\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graphvite/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[0;32m~/anaconda3/envs/graphvite/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    307\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[1;32m    308\u001b[0m                         \u001b[0;34m'must be a string or Tensor. (%s)'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                         (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[1;32m    310\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument 8948.672 has invalid type <class 'numpy.float32'>, must be a string or Tensor. (Can not convert a float32 into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "#         print(batch_y.shape, batch_x.shape)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                      Y: mnist.test.labels}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.x",
   "language": "python",
   "name": "brenda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
